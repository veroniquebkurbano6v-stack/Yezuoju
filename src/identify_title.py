from typing import List, Dict, Any
from dotenv import load_dotenv
import os
import ast
import json
from pathlib import Path
import time
import hashlib
import traceback

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class TitleIdentifier:
    def __init__(self):
        # åŠ è½½æ ‡ç‚¹ç¬¦å·åˆ—è¡¨
        try:
            env_punctuation = os.getenv("all_punctuation_list", "[]")
            print(f"ğŸ” è°ƒè¯•ï¼šç¯å¢ƒå˜é‡all_punctuation_liståŸå§‹å€¼é•¿åº¦: {len(env_punctuation)}")
            self.all_punctuation_list = ast.literal_eval(env_punctuation)
            print(f"âœ… è°ƒè¯•ï¼šæ ‡ç‚¹ç¬¦å·åˆ—è¡¨æˆåŠŸåŠ è½½ï¼Œæ•°é‡: {len(self.all_punctuation_list)}")
        except (SyntaxError, ValueError) as e:
            print(f"âŒ è°ƒè¯•ï¼šæ ‡ç‚¹ç¬¦å·åˆ—è¡¨åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨: {e}")
            self.all_punctuation_list = ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', 'â€¦', 'â€¦â€¦', 'â€”â€”', 'â€•â€•', ',', 'ï¼Œ', 'ã€', ';', 'ï¼›', ':', 'ï¼š']
        
        # é¢„åŠ è½½å…³é”®è¯åˆ—è¡¨ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨æ—¶é‡å¤è§£æç¯å¢ƒå˜é‡
        try:
            self.chinese_keywords = ast.literal_eval(os.getenv("chinese_title_keywords", "[]"))
            self.japanese_keywords = ast.literal_eval(os.getenv("japanese_title_keywords", "[]"))
            self.english_keywords = ast.literal_eval(os.getenv("english_title_keywords", "[]"))
        except (SyntaxError, ValueError) as e:
            print(f"âŒ å…³é”®è¯åˆ—è¡¨åŠ è½½å¤±è´¥: {e}")
            self.chinese_keywords = []
            self.japanese_keywords = []
            self.english_keywords = []

        # åŠ è½½é•¿åº¦é™åˆ¶é…ç½®
        self.chinese_max_chars = int(os.getenv("chinese_title_max_chars", 7))
        self.japanese_max_chars = int(os.getenv("japanese_title_max_chars", 8))
        self.english_max_words = int(os.getenv("english_title_max_words", 7))
        
    def _get_keywords(self, language: str) -> List[str]:
        """æ ¹æ®è¯­è¨€è·å–å…³é”®è¯åˆ—è¡¨"""
        if language == "Chinese":
            return self.chinese_keywords
        elif language == "Japanese":
            return self.japanese_keywords
        elif language == "English":
            return self.english_keywords
        else:
            return []
    def check_title_length(self, text: str, language: str) -> bool:
        """æ£€æŸ¥æ ‡é¢˜é•¿åº¦æ˜¯å¦ç¬¦åˆè¯­è¨€é™åˆ¶"""
        has_chinese = any('\u4e00' <= c <= '\u9fff' for c in text)
        has_japanese = any('\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff' for c in text)
        
        if has_chinese:
            return len(text) <= self.chinese_max_chars
        elif has_japanese:
            return len(text) <= self.japanese_max_chars
        elif language == "English":
            words = text.split()
            return len(words) <= self.english_max_words
        else:
            return len(text) <= self.chinese_max_chars
    
    def check_text_centered(self, block, page_width: float, tolerance: float = 0.1) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬å—æ˜¯å¦æ°´å¹³å±…ä¸­
        
        Args:
            block: æ–‡æœ¬å—å¯¹è±¡ï¼Œå‡è®¾æœ‰x0, x1å±æ€§è¡¨ç¤ºæ°´å¹³è¾¹ç•Œ
            page_width: é¡µé¢æ€»å®½åº¦
            tolerance: å±…ä¸­å®¹å¿åº¦ï¼ˆé»˜è®¤ä¸º0.1ï¼Œå³é¡µé¢å®½åº¦çš„10%ï¼‰
            
        Returns:
            bool: æ˜¯å¦æ°´å¹³å±…ä¸­
        """
        # å¦‚æœæ–‡æœ¬å—æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œè¿”å›False
        if not hasattr(block, 'x0') or not hasattr(block, 'x1'):
            return False
        
        # è®¡ç®—æ–‡æœ¬å—çš„ä¸­å¿ƒä½ç½®
        block_center = (block.x0 + block.x1) / 2
        
        # è®¡ç®—é¡µé¢çš„å±…ä¸­åŒºåŸŸï¼ˆé¡µé¢å®½åº¦çš„40%-60%ä¸ºå±…ä¸­åŒºåŸŸï¼‰
        center_start = page_width * (0.5 - tolerance)
        center_end = page_width * (0.5 + tolerance)
        
        return center_start <= block_center <= center_end
    
    def identify_title(self, pages: List[Any]) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«PDFä¸­çš„æ ‡é¢˜
        
        Args:
            pages: PDFLoader.load_pdf() è¿”å›çš„é¡µé¢åˆ—è¡¨
            
        Returns:
            åŒ…å«æ ‡é¢˜åç§°å’Œèµ·å§‹é¡µçš„åˆ—è¡¨
        """
        if not pages:
            return []
        
        # è·å–è¯­è¨€ç±»å‹
        language = pages[0].language
        print(f"è¯†åˆ«è¯­è¨€: {language}")
        
        # æ ¹æ®è¯­è¨€è·å–å…³é”®è¯ï¼ˆå·²åœ¨æ„é€ å™¨é¢„åŠ è½½ï¼‰
        keywords = self._get_keywords(language)
        print(f"å…³é”®è¯åˆ—è¡¨: {keywords}")
        
        titles = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šéå†å‰10é¡µï¼ŒæŸ¥æ‰¾åŒ…å«å…³é”®è¯ä¸”ä¸å«æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬å—
        first_10_pages = pages[:10]
        print(f"ç¬¬ä¸€é˜¶æ®µï¼šéå†å‰ {len(first_10_pages)} é¡µ")
        
        for page in first_10_pages:
            page_num = page.page_number
            text_blocks = page.text_blocks  # æ‰€æœ‰æ–‡æœ¬å—
            
            # ä¼°ç®—é¡µé¢å®½åº¦ï¼ˆå¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
            page_width = 612.0  # é»˜è®¤é¡µé¢å®½åº¦
            try:
                if hasattr(page, 'width') and page.width > 0:
                    page_width = page.width
                elif hasattr(page, 'mediabox') and hasattr(page.mediabox, 'width'):
                    page_width = page.mediabox.width
                elif text_blocks:
                    max_x = max(block.x1 for block in text_blocks if hasattr(block, 'x1'))
                    if max_x > 0:
                        page_width = max_x
            except:
                pass  # ä½¿ç”¨é»˜è®¤å€¼
            
            for block in text_blocks:
                text = block.text.strip()
                
                if not text:
                    continue
                
                # ã€é¦–è¦ä¸”ä¸å¯æ›´æ”¹ã€‘æ£€æŸ¥æ˜¯å¦ä¸å«æ ‡ç‚¹ç¬¦å· - ç¬¬ä¸€ä¸ªå¼ºåˆ¶æ€§æ¡ä»¶
                has_punctuation = any(punc in text for punc in self.all_punctuation_list)
                if has_punctuation:
                    continue  # è·³è¿‡å«æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬å—
                
                # æ£€æŸ¥æ°´å¹³å±…ä¸­
                is_centered = self.check_text_centered(block, page_width)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                contains_keyword = any(keyword in text for keyword in keywords)
                
                # æ£€æŸ¥é•¿åº¦é™åˆ¶
                length_ok = self.check_title_length(text, language)
                
                # æ ‡é¢˜å€™é€‰åº”åŒæ—¶æ»¡è¶³ï¼šæ— æ ‡ç‚¹ç¬¦å· AND (åŒ…å«å…³é”®è¯ OR æ°´å¹³å±…ä¸­) AND é•¿åº¦åˆé€‚
                if not has_punctuation and (contains_keyword or is_centered) and length_ok:
                    # ç¬¦åˆæ¡ä»¶ï¼Œè®°å½•æ ‡é¢˜
                    titles.append({
                        "title": text,
                        "start_page": page_num
                    })
                    print(f"æ‰¾åˆ°æ ‡é¢˜ï¼š'{text}'ï¼Œé¡µç ï¼š{page_num}")
                    break  # æ¯ä¸€é¡µåªèƒ½æœ‰ä¸€ä¸ªæ ‡é¢˜
        
        print(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œå…±æ‰¾åˆ° {len(titles)} ä¸ªæ ‡é¢˜")
        
        # å¦‚æœç¬¬ä¸€é˜¶æ®µæ‰¾åˆ°äº†æ ‡é¢˜ï¼Œç»§ç»­éå†å‰©ä½™é¡µé¢æŸ¥æ‰¾æ›´å¤šæ ‡é¢˜
        if titles:
            print("ç¬¬ä¸€é˜¶æ®µæ‰¾åˆ°æ ‡é¢˜ï¼Œç»§ç»­éå†å‰©ä½™é¡µé¢...")
            
            # ç»§ç»­éå†ç¬¬11é¡µå¼€å§‹çš„å‰©ä½™é¡µé¢
            remaining_pages = pages[10:]
            for page in remaining_pages:
                page_num = page.page_number
                text_blocks = page.text_blocks  # æ‰€æœ‰æ–‡æœ¬å—
                
                # ä¼°ç®—é¡µé¢å®½åº¦
                page_width = 612.0  # é»˜è®¤é¡µé¢å®½åº¦
                try:
                    if hasattr(page, 'width') and page.width > 0:
                        page_width = page.width
                    elif hasattr(page, 'mediabox') and hasattr(page.mediabox, 'width'):
                        page_width = page.mediabox.width
                    elif text_blocks:
                        max_x = max(block.x1 for block in text_blocks if hasattr(block, 'x1'))
                        if max_x > 0:
                            page_width = max_x
                except:
                    pass  # ä½¿ç”¨é»˜è®¤å€¼
                
                for block in text_blocks:
                    text = block.text.strip()
                    
                    if not text:
                        continue
                    
                    # ã€é¦–è¦ä¸”ä¸å¯æ›´æ”¹ã€‘æ£€æŸ¥æ˜¯å¦ä¸å«æ ‡ç‚¹ç¬¦å·
                    has_punctuation = any(punc in text for punc in self.all_punctuation_list)
                    if has_punctuation:
                        continue  # è·³è¿‡å«æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬å—
                    
                    # æ£€æŸ¥æ°´å¹³å±…ä¸­
                    is_centered = self.check_text_centered(block, page_width)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                    contains_keyword = any(keyword in text for keyword in keywords)
                    
                    # æ£€æŸ¥é•¿åº¦é™åˆ¶
                    length_ok = self.check_title_length(text, language)
                    
                    if not has_punctuation and (contains_keyword or is_centered) and length_ok:
                        # ç¬¦åˆæ¡ä»¶ï¼Œè®°å½•æ ‡é¢˜
                        titles.append({
                            "title": text,
                            "start_page": page_num
                        })
                        print(f"ç»§ç»­éå†æ‰¾åˆ°æ ‡é¢˜ï¼š'{text}'ï¼Œé¡µç ï¼š{page_num}")
                        break  # æ¯ä¸€é¡µåªèƒ½æœ‰ä¸€ä¸ªæ ‡é¢˜
            
            print(f"å®Œæ•´éå†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(titles)} ä¸ªæ ‡é¢˜")
            return self._calculate_end_pages(titles, pages)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœå‰10é¡µæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œåˆ‡æ¢æ£€ç´¢æ¨¡å¼
        print("ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨é¢„å¤‡æ ‡é¢˜æ¨¡å¼")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å—ä¸­ä¸å«æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬
        candidate_titles = []
        for page in pages:
            page_num = page.page_number
            text_blocks = page.text_blocks  # æ‰€æœ‰æ–‡æœ¬å—
            
            # ä¼°ç®—é¡µé¢å®½åº¦
            page_width = 612.0  # é»˜è®¤é¡µé¢å®½åº¦
            try:
                if hasattr(page, 'width') and page.width > 0:
                    page_width = page.width
                elif hasattr(page, 'mediabox') and hasattr(page.mediabox, 'width'):
                    page_width = page.mediabox.width
                elif text_blocks:
                    max_x = max(block.x1 for block in text_blocks if hasattr(block, 'x1'))
                    if max_x > 0:
                        page_width = max_x
            except:
                pass  # ä½¿ç”¨é»˜è®¤å€¼
            
            # åŠ è½½å…³é”®è¯
            keywords = self._get_keywords(language)
            
            for block in text_blocks:
                text = block.text.strip()
                
                if not text:
                    continue
                
                # ã€é¦–è¦ä¸”ä¸å¯æ›´æ”¹ã€‘æ£€æŸ¥æ˜¯å¦ä¸å«æ ‡ç‚¹ç¬¦å·
                has_punctuation = any(punc in text for punc in self.all_punctuation_list)
                if has_punctuation:
                    print(f"ğŸ” è°ƒè¯•ï¼šç¬¬äºŒé˜¶æ®µè·³è¿‡å«æ ‡ç‚¹æ–‡æœ¬ '{text}' (é¡µç : {page_num})")
                    continue  # è·³è¿‡å«æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬å—
                
                # æ£€æŸ¥æ°´å¹³å±…ä¸­
                is_centered = self.check_text_centered(block, page_width)
                
                # æ£€æŸ¥é•¿åº¦é™åˆ¶
                length_ok = self.check_title_length(text, language)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                contains_keyword = any(keyword in text for keyword in keywords)
                
                # ã€å…³é”®ä¿®å¤ã€‘æ ‡é¢˜å€™é€‰åº”åŒæ—¶æ»¡è¶³ï¼šæ— æ ‡ç‚¹ç¬¦å· AND (åŒ…å«å…³é”®è¯ OR æ°´å¹³å±…ä¸­) AND é•¿åº¦åˆé€‚
                if not has_punctuation and (contains_keyword or is_centered) and length_ok:
                    # æ·»åŠ åˆ°é¢„å¤‡æ ‡é¢˜åˆ—è¡¨
                    candidate_titles.append({
                        "title": text,
                        "start_page": page_num
                    })
                    print(f"ğŸ” è°ƒè¯•ï¼šç¬¬äºŒé˜¶æ®µé€šè¿‡å€™é€‰æ–‡æœ¬ '{text}' (é¡µç : {page_num}, å…³é”®è¯: {contains_keyword}, å±…ä¸­: {is_centered})")
                else:
                    # è°ƒè¯•ï¼šè®°å½•å¤±è´¥åŸå› 
                    reasons = []
                    if has_punctuation:
                        reasons.append("å«æ ‡ç‚¹")
                    if not (contains_keyword or is_centered):
                        reasons.append("æ— å…³é”®è¯ä¸”ä¸å±…ä¸­")
                    if not length_ok:
                        reasons.append("é•¿åº¦ä¸åˆè§„")
                    print(f"ğŸ” è°ƒè¯•ï¼šç¬¬äºŒé˜¶æ®µæ‹’ç»æ–‡æœ¬ '{text}' (é¡µç : {page_num}, åŸå› : {', '.join(reasons)})")
        
        if not candidate_titles:
            print("æœªæ‰¾åˆ°ä»»ä½•æ ‡é¢˜")
            return []
        
        # ç»Ÿè®¡é¢„å¤‡æ ‡é¢˜çš„å‡ºç°æ¬¡æ•°
        title_counts = {}
        for title_info in candidate_titles:
            title = title_info["title"]
            if title in title_counts:
                title_counts[title] += 1
            else:
                title_counts[title] = 1
        
        # æ‰¾å‡ºé‡å¤çš„æ ‡é¢˜ï¼ˆå‡ºç°æ¬¡æ•°å¤§äº1ï¼‰
        duplicate_titles = [title for title, count in title_counts.items() if count > 1]
        
        if duplicate_titles:
            # å¯¹äºæ¯ä¸ªé‡å¤çš„æ ‡é¢˜ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„
            final_titles = []
            seen_titles = set()
            for title_info in candidate_titles:
                if title_info["title"] in duplicate_titles and title_info["title"] not in seen_titles:
                    final_titles.append(title_info)
                    seen_titles.add(title_info["title"])
            
            result_titles = self._calculate_end_pages(final_titles, pages)
        else:
            # å¦‚æœæ²¡æœ‰é‡å¤çš„æ ‡é¢˜ï¼Œä½¿ç”¨æ‰€æœ‰é¢„å¤‡æ ‡é¢˜
            result_titles = self._calculate_end_pages(candidate_titles, pages)
        
        # ã€æœ€ç»ˆéªŒè¯ã€‘å¼ºåˆ¶è¿›è¡Œæ ‡ç‚¹ç¬¦å·å¤æŸ¥
        print(f"\nğŸ›¡ï¸ æœ€ç»ˆéªŒè¯ï¼šå¯¹ {len(result_titles)} ä¸ªæ ‡é¢˜è¿›è¡Œæ ‡ç‚¹ç¬¦å·æ£€æŸ¥...")
        validated_titles = []
        
        for title_info in result_titles:
            title_text = title_info["title"]
            has_punctuation = any(punc in title_text for punc in self.all_punctuation_list)
            
            if has_punctuation:
                matched_puncs = [punc for punc in self.all_punctuation_list if punc in title_text]
                print(f"âš ï¸  ç§»é™¤å«æ ‡ç‚¹æ ‡é¢˜: '{title_text}' (é¡µç : {title_info['start_page']}-{title_info['end_page']}, åŒ¹é…æ ‡ç‚¹: {matched_puncs})")
                continue  # è·³è¿‡å«æ ‡ç‚¹çš„æ ‡é¢˜
            
            validated_titles.append(title_info)
            print(f"âœ… éªŒè¯é€šè¿‡: '{title_text}' (é¡µç : {title_info['start_page']}-{title_info['end_page']})")
        
        print(f"ğŸ›¡ï¸ æœ€ç»ˆéªŒè¯å®Œæˆï¼š{len(result_titles)} -> {len(validated_titles)} ä¸ªæ ‡é¢˜")
        return validated_titles
    
    def _calculate_end_pages(self, titles: List[Dict[str, Any]], pages: List[Any]) -> List[Dict[str, Any]]:
        """
        è®¡ç®—æ ‡é¢˜çš„ç»“æŸé¡µç 
        
        Args:
            titles: åŒ…å«æ ‡é¢˜å’Œèµ·å§‹é¡µçš„åˆ—è¡¨
            pages: PDFé¡µé¢åˆ—è¡¨
            
        Returns:
            åŒ…å«æ ‡é¢˜ã€èµ·å§‹é¡µå’Œç»“æŸé¡µçš„åˆ—è¡¨
        """
        if not titles:
            return titles
            
        # æŒ‰èµ·å§‹é¡µæ’åº
        sorted_titles = sorted(titles, key=lambda x: x["start_page"])
        
        for i, title_info in enumerate(sorted_titles):
            start_page = title_info["start_page"]
            
            if i < len(sorted_titles) - 1:
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ ‡é¢˜ï¼Œç»“æŸé¡µæ˜¯ä¸‹ä¸€ä¸ªæ ‡é¢˜èµ·å§‹é¡µçš„å‰ä¸€é¡µ
                next_start_page = sorted_titles[i + 1]["start_page"]
                end_page = next_start_page - 1
            else:
                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ ‡é¢˜ï¼Œç»“æŸé¡µæ˜¯æ–‡æ¡£çš„æœ€åä¸€é¡µ
                end_page = max(page.page_number for page in pages)
            
            title_info["end_page"] = end_page
        
        return sorted_titles
    
    def _build_page_title_map(self, title_blocks: List[Dict[str, Any]], total_pages: int) -> List[str]:
        """
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ„å»ºé¡µé¢æ ‡é¢˜æ˜ å°„è¡¨ï¼ŒO(N+M)å¤æ‚åº¦
        
        Args:
            title_blocks: æ ‡é¢˜åŒºå—åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«start_pageå’Œend_page
            total_pages: æ€»é¡µæ•°
            
        Returns:
            é¡µé¢æ ‡é¢˜æ˜ å°„åˆ—è¡¨ï¼Œç´¢å¼•ä¸ºé¡µç -1
        """
        # åˆå§‹åŒ–é¡µé¢æ ‡é¢˜æ˜ å°„è¡¨ï¼Œç´¢å¼•0å¯¹åº”ç¬¬1é¡µ
        page_title_map = [""] * total_pages
        
        # ä¸ºæ¯ä¸ªæ ‡é¢˜åŒºå—åˆ†é…é¡µé¢
        for block in title_blocks:
            start_page = block["start_page"]
            end_page = block["end_page"]
            title = block["title"]
            
            # ç¡®ä¿é¡µç åœ¨æœ‰æ•ˆèŒƒå›´å†…
            for page_num in range(start_page, end_page + 1):
                if 1 <= page_num <= total_pages:
                    page_title_map[page_num - 1] = title
        
        return page_title_map


def get_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()
def process_pdf_to_page_titles(pdf_path: str, title_identifier: TitleIdentifier, pdf_loader) -> dict:
    """å¤„ç†å•ä¸ªPDFï¼Œä¸ºæ¯ä¸€é¡µç¡®å®šç« èŠ‚æ ‡é¢˜"""
    try:
        # åŠ è½½PDFé¡µé¢
        pages = pdf_loader.load_pdf(pdf_path)
        if not pages:
            return None
        
        # è¯†åˆ«æ ‡é¢˜åŒºå—
        title_blocks = title_identifier.identify_title(pages)
        print(title_blocks)
        
        # æ„å»ºé¡µé¢æ ‡é¢˜æ˜ å°„ï¼ˆè‹¥æ— æ ‡é¢˜åˆ™å…¨éƒ¨ä¸ºç©ºï¼‰
        total_pages = max(page.page_number for page in pages)
        page_title_map = title_identifier._build_page_title_map(title_blocks, total_pages) if title_blocks else [""] * total_pages
        
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆæ–‡æœ¬å—åˆ—è¡¨ï¼ˆdocument_chunksï¼‰
        document_chunks = []
        chunk_counter = 0

        # æœ¬åœ°åˆ†å—å‡½æ•°ï¼Œä¿æŒç®€å•ç¨³å®š
        def local_chunk_text(full_text: str, chunk_size: int = 512, overlap: int = 50):
            if not full_text:
                return []
            chunks = []
            start = 0
            text_len = len(full_text)
            while start < text_len:
                end = start + chunk_size
                if end < text_len:
                    # å°è¯•åœ¨å¥å·æˆ–æ¢è¡Œå¤„ä¼˜å…ˆæ–­å¥
                    last_period = full_text.rfind('ã€‚', start, end)
                    last_period_en = full_text.rfind('.', start, end)
                    last_break = max(last_period, last_period_en)
                    if last_break > start + chunk_size // 2:
                        end = last_break + 1
                chunk = full_text[start:end].strip()
                if chunk:
                    chunks.append(chunk)
                start = end - overlap if end < text_len else end
            return chunks

        for page in pages:
            page_num = page.page_number
            section_title = page_title_map[page_num - 1] if page_num <= len(page_title_map) else ""
            
            # è·å–é¡µé¢å®Œæ•´æ–‡æœ¬ï¼Œç„¶ååˆ†å—
            full_text = page.get_full_text()
            if not full_text or not full_text.strip():
                continue

            text_chunks = local_chunk_text(full_text, chunk_size=512, overlap=50)

            for i, chunk in enumerate(text_chunks):
                chunk_id = f"{Path(pdf_path).stem}_p{page_num:03d}_c{i:03d}"
                chunk_counter += 1

                # è·å–æ–‡æœ¬å—åæ ‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                coordinates = None
                try:
                    if hasattr(page, 'text_blocks') and page.text_blocks:
                        block = page.text_blocks[0]
                        coordinates = {
                            "x0": getattr(block, "x0", None),
                        "y0": getattr(block, "y0", None),
                        "x1": getattr(block, "x1", None),
                        "y1": getattr(block, "y1", None)
                    }
                except Exception:
                    coordinates = None

                document_chunks.append({
                    "id": chunk_id,
                    "text": chunk,
                    "page_number": page_num,
                    "section_title": section_title,
                    "chunk_index": i,
                    "total_chunks_in_page": len(text_chunks),
                    "coordinates": coordinates,
                    "text_hash": hashlib.md5(chunk.encode()).hexdigest()[:16]
                })
        
        # æå–è¯­è¨€ä¿¡æ¯
        language = pages[0].language if pages else "Unknown"
        
        # æ„å»ºç»“æœç»“æ„ï¼Œç¬¦åˆç”¨æˆ·æŒ‡å®šæ ¼å¼ï¼ˆä¸åŒ…å« _cacheï¼‰
        result = {
            "parent_document": {
                "filename": Path(pdf_path).name,
                "file_path": pdf_path,
                "total_pages": len(pages),
                "language": language,
                "total_chunks": chunk_counter,
                "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "document_chunks": document_chunks
        }
        
        return result
        
    except Exception as e:
        # æ‰“å°å®Œæ•´ traceback ä»¥ä¾¿å®šä½é—®é¢˜æ¥æºï¼ˆä¾‹å¦‚ "'str' object is not callable"ï¼‰
        tb = traceback.format_exc()
        print(f"å¤„ç†PDFå¤±è´¥ {pdf_path}: {tb}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“š PDFç« èŠ‚æ ‡é¢˜è¯†åˆ«å·¥å…·")
    print("=" * 60)
    
    # å®šä¹‰è·¯å¾„
    source_dir = Path("src/data/source")
    output_dir = Path("src/data/pages_title")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç»„ä»¶
    title_identifier = TitleIdentifier()
    from src.data.pdf_loader import PDFLoader
    pdf_loader = PDFLoader()
    
    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = list(source_dir.rglob("*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {source_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    processed_count = 0
    skipped_count = 0
    failed_count = 0
    
    for pdf_path in pdf_files:
        try:
            print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_path.relative_to(source_dir)}")
            
            # ç”Ÿæˆå¯¹åº”çš„JSONç¼“å­˜æ–‡ä»¶è·¯å¾„
            json_filename = f"{pdf_path.stem}_titles.json"
            json_path = output_dir / json_filename
            
            # ç®€åŒ–ç¼“å­˜é€»è¾‘ï¼šå¦‚æœ JSON å·²å­˜åœ¨åˆ™è·³è¿‡ï¼ˆé¿å…å¤æ‚çš„å“ˆå¸Œæ¯”è¾ƒï¼‰
            if json_path.exists():
                print(f"  âœ… ç¼“å­˜å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                skipped_count += 1
                continue
            
            # å¤„ç†PDF
            result = process_pdf_to_page_titles(str(pdf_path), title_identifier, pdf_loader)
            
            if result is None:
                print(f"  âŒ å¤„ç†å¤±è´¥")
                failed_count += 1
                continue
            
            # ä¿å­˜ç»“æœ
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼šå…¼å®¹æ—§/æ–°ä¸¤ç§ JSON ç»“æ„
            if "document_chunks" in result and isinstance(result.get("document_chunks"), list):
                unique_titles = len(set(chunk.get("section_title", "") for chunk in result["document_chunks"] if chunk.get("section_title")))
            elif "page_sections" in result and isinstance(result.get("page_sections"), list):
                unique_titles = len(set(section.get("section_title", "") for section in result["page_sections"]))
                unique_titles = len(set(chunk.get("section_title", "") for chunk in result["document_chunks"] if chunk.get("section_title")))
            else:
                unique_titles = 0

            total_pages = result["parent_document"].get("total_pages", 0)
            
            print(f"  âœ… å¤„ç†å®Œæˆ")
            print(f"     ğŸ“„ æ€»é¡µæ•°: {total_pages}")
            print(f"     ğŸ“‘ ç« èŠ‚æ•°: {unique_titles}")
            print(f"     ğŸ’¾ ç¼“å­˜æ–‡ä»¶: {json_filename}")
            
            processed_count += 1
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
        
        print()  # ç©ºè¡Œåˆ†éš”
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print("ğŸ“Š å¤„ç†å®Œæˆæ±‡æ€»:")
    print(f"  âœ… æˆåŠŸå¤„ç†: {processed_count}")
    print(f"  â­ï¸  è·³è¿‡(ç¼“å­˜): {skipped_count}")
    print(f"  âŒ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"  ğŸ“ æ€»è®¡: {len(pdf_files)}")
    print()
    
    if processed_count > 0:
        print("ğŸ’¡ ç”Ÿæˆçš„JSONæ–‡ä»¶å¯ç›´æ¥ç”¨äºåˆ›å»ºDocumentChunkï¼Œç»“æ„å¦‚ä¸‹:")
        print("  - parent_document: PDFåŸºæœ¬ä¿¡æ¯ (åŒ…å« processing_date å’Œ total_chunks)")
        print("  - document_chunks: æ¯ä¸ªæ–‡æœ¬å—çš„è¯¦ç»†ä¿¡æ¯ (id, text, page_number, section_title, text_hash ç­‰)")

if __name__ == "__main__":
    import multiprocessing
    import os

    # Windows æ”¯æŒï¼šåœ¨ spawn æ¨¡å¼ä¸‹å®‰å…¨å¯åŠ¨å­è¿›ç¨‹
    multiprocessing.freeze_support()
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®è¿‡å¯åŠ¨æ–¹æ³•ï¼Œè·³è¿‡
        pass

    # é™ä½å¹¶è¡Œåº“çº¿ç¨‹æ•°ï¼Œå‡å°‘ä¸å¤šè¿›ç¨‹çš„å†²çª
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

    main()